#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå¢å¼ºåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
ç»“åˆäººå·¥æ™ºèƒ½å’Œæ‰€æœ‰æµ‹è¯•è®¾è®¡æ–¹æ³•ï¼Œç”Ÿæˆå®Œå–„å…¨é¢çš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
"""

import re
import itertools
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# å¯¼å…¥åŸºç¡€ç”Ÿæˆå™¨
from test_case_generator import TestCaseGenerator, TestCase, Priority, TestMethod
from comprehensive_test_generator import RequirementAnalysis

class AITestMethod(Enum):
    """AIå¢å¼ºæµ‹è¯•æ–¹æ³•"""
    INTELLIGENT_BOUNDARY = "AIæ™ºèƒ½è¾¹ç•Œåˆ†æ"
    SEMANTIC_EQUIVALENCE = "è¯­ä¹‰ç­‰ä»·ç±»åˆ’åˆ†"
    CONTEXT_SCENARIO = "ä¸Šä¸‹æ–‡åœºæ™¯åˆ†æ"
    PREDICTIVE_ERROR = "é¢„æµ‹æ€§é”™è¯¯åˆ†æ"
    ADAPTIVE_COMBINATION = "è‡ªé€‚åº”ç»„åˆæµ‹è¯•"
    RISK_BASED_PRIORITY = "åŸºäºé£é™©çš„ä¼˜å…ˆçº§"
    COVERAGE_OPTIMIZATION = "è¦†ç›–åº¦ä¼˜åŒ–"
    INTELLIGENT_REGRESSION = "æ™ºèƒ½å›å½’æµ‹è¯•"

@dataclass
class AIAnalysisResult:
    """AIåˆ†æç»“æœ"""
    complexity_score: float  # å¤æ‚åº¦è¯„åˆ† 0-1
    risk_areas: List[str]    # é£é™©åŒºåŸŸ
    critical_paths: List[str]  # å…³é”®è·¯å¾„
    data_patterns: List[Dict]  # æ•°æ®æ¨¡å¼
    business_rules: List[Dict]  # ä¸šåŠ¡è§„åˆ™
    integration_points: List[str]  # é›†æˆç‚¹
    performance_concerns: List[str]  # æ€§èƒ½å…³æ³¨ç‚¹
    security_risks: List[str]  # å®‰å…¨é£é™©
    usability_factors: List[str]  # å¯ç”¨æ€§å› ç´ 

class AITestCaseGenerator(TestCaseGenerator):
    """AIå¢å¼ºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨"""
    
    def __init__(self, custom_headers: Optional[Dict[str, str]] = None):
        super().__init__(custom_headers)
        self.ai_analysis: Optional[AIAnalysisResult] = None
        self.knowledge_base = self._load_knowledge_base()
        self.pattern_library = self._load_pattern_library()
        
    def _load_knowledge_base(self) -> Dict:
        """åŠ è½½æµ‹è¯•çŸ¥è¯†åº“"""
        return {
            "common_vulnerabilities": [
                "SQLæ³¨å…¥", "XSSè·¨ç«™è„šæœ¬", "CSRFè·¨ç«™è¯·æ±‚ä¼ªé€ ", "æ–‡ä»¶ä¸Šä¼ æ¼æ´",
                "æƒé™ç»•è¿‡", "ä¼šè¯åŠ«æŒ", "ç¼“å†²åŒºæº¢å‡º", "è·¯å¾„éå†"
            ],
            "performance_patterns": [
                "é«˜å¹¶å‘è®¿é—®", "å¤§æ•°æ®é‡å¤„ç†", "é•¿æ—¶é—´è¿è¡Œ", "å†…å­˜æ³„æ¼",
                "æ•°æ®åº“è¿æ¥æ± ", "ç¼“å­˜å¤±æ•ˆ", "ç½‘ç»œå»¶è¿Ÿ", "èµ„æºç«äº‰"
            ],
            "usability_issues": [
                "å“åº”æ—¶é—´è¿‡é•¿", "ç•Œé¢ä¸å‹å¥½", "æ“ä½œå¤æ‚", "é”™è¯¯æç¤ºä¸æ¸…",
                "æ— éšœç¢è®¿é—®", "ç§»åŠ¨ç«¯é€‚é…", "æµè§ˆå™¨å…¼å®¹", "å›½é™…åŒ–æ”¯æŒ"
            ],
            "integration_risks": [
                "APIç‰ˆæœ¬ä¸å…¼å®¹", "æ•°æ®æ ¼å¼ä¸åŒ¹é…", "è¶…æ—¶å¤„ç†", "é”™è¯¯ä¼ æ’­",
                "äº‹åŠ¡ä¸€è‡´æ€§", "æ¶ˆæ¯é˜Ÿåˆ—", "ç¬¬ä¸‰æ–¹æœåŠ¡", "æ•°æ®åŒæ­¥"
            ],
            "business_patterns": {
                "ç”µå•†": ["è´­ç‰©è½¦", "æ”¯ä»˜", "åº“å­˜", "è®¢å•", "ç”¨æˆ·", "å•†å“"],
                "é‡‘è": ["è´¦æˆ·", "äº¤æ˜“", "é£æ§", "åˆè§„", "æ¸…ç®—", "æŠ¥è¡¨"],
                "æ•™è‚²": ["è¯¾ç¨‹", "å­¦å‘˜", "è€ƒè¯•", "æˆç»©", "èµ„æº", "äº’åŠ¨"],
                "åŒ»ç–—": ["æ‚£è€…", "è¯Šæ–­", "å¤„æ–¹", "æ£€æŸ¥", "ç—…å†", "é¢„çº¦"]
            }
        }
    
    def _load_pattern_library(self) -> Dict:
        """åŠ è½½æµ‹è¯•æ¨¡å¼åº“"""
        return {
            "input_patterns": {
                "email": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
                "phone": r"^1[3-9]\d{9}$",
                "id_card": r"^\d{17}[\dXx]$",
                "password": r"^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)[a-zA-Z\d@$!%*?&]{8,}$"
            },
            "boundary_patterns": {
                "string_length": [0, 1, 255, 256, 1000, 4000, 65535],
                "numeric_range": [-2147483648, -1, 0, 1, 2147483647],
                "date_range": ["1900-01-01", "2000-01-01", "2024-12-31", "2099-12-31"]
            },
            "error_patterns": [
                "ç©ºå€¼å¤„ç†", "ç‰¹æ®Šå­—ç¬¦", "è¶…é•¿è¾“å…¥", "å¹¶å‘å†²çª",
                "ç½‘ç»œä¸­æ–­", "æœåŠ¡ä¸å¯ç”¨", "æƒé™ä¸è¶³", "èµ„æºè€—å°½"
            ]
        }
    
    def ai_analyze_requirements(self, requirement_text: str) -> AIAnalysisResult:
        """AIæ™ºèƒ½åˆ†æéœ€æ±‚"""
        # å¤æ‚åº¦è¯„åˆ†
        complexity_score = self._calculate_complexity(requirement_text)
        
        # è¯†åˆ«é£é™©åŒºåŸŸ
        risk_areas = self._identify_risk_areas(requirement_text)
        
        # æå–å…³é”®è·¯å¾„
        critical_paths = self._extract_critical_paths(requirement_text)
        
        # åˆ†ææ•°æ®æ¨¡å¼
        data_patterns = self._analyze_data_patterns(requirement_text)
        
        # æå–ä¸šåŠ¡è§„åˆ™
        business_rules = self._extract_business_rules(requirement_text)
        
        # è¯†åˆ«é›†æˆç‚¹
        integration_points = self._identify_integration_points(requirement_text)
        
        # æ€§èƒ½å…³æ³¨ç‚¹
        performance_concerns = self._identify_performance_concerns(requirement_text)
        
        # å®‰å…¨é£é™©
        security_risks = self._identify_security_risks(requirement_text)
        
        # å¯ç”¨æ€§å› ç´ 
        usability_factors = self._identify_usability_factors(requirement_text)
        
        self.ai_analysis = AIAnalysisResult(
            complexity_score=complexity_score,
            risk_areas=risk_areas,
            critical_paths=critical_paths,
            data_patterns=data_patterns,
            business_rules=business_rules,
            integration_points=integration_points,
            performance_concerns=performance_concerns,
            security_risks=security_risks,
            usability_factors=usability_factors
        )
        
        return self.ai_analysis
    
    def _calculate_complexity(self, text: str) -> float:
        """è®¡ç®—éœ€æ±‚å¤æ‚åº¦"""
        factors = {
            "length": len(text) / 10000,  # æ–‡æœ¬é•¿åº¦
            "rules": len(re.findall(r'è§„åˆ™|æ¡ä»¶|å¦‚æœ|å½“.*æ—¶', text)) / 10,  # ä¸šåŠ¡è§„åˆ™æ•°é‡
            "entities": len(re.findall(r'ç”¨æˆ·|è®¢å•|å•†å“|è´¦æˆ·|æ•°æ®', text)) / 10,  # å®ä½“æ•°é‡
            "integrations": len(re.findall(r'æ¥å£|API|ç¬¬ä¸‰æ–¹|é›†æˆ', text)) / 5,  # é›†æˆç‚¹
            "states": len(re.findall(r'çŠ¶æ€|é˜¶æ®µ|æ­¥éª¤', text)) / 8  # çŠ¶æ€æ•°é‡
        }
        
        # åŠ æƒè®¡ç®—å¤æ‚åº¦
        weights = {"length": 0.1, "rules": 0.3, "entities": 0.2, "integrations": 0.2, "states": 0.2}
        complexity = sum(min(factors[k], 1.0) * weights[k] for k in factors)
        
        return min(complexity, 1.0)
    
    def _identify_risk_areas(self, text: str) -> List[str]:
        """è¯†åˆ«é£é™©åŒºåŸŸ"""
        risks = []
        
        # å®‰å…¨é£é™©
        if re.search(r'ç™»å½•|å¯†ç |è®¤è¯|æƒé™', text, re.IGNORECASE):
            risks.append("èº«ä»½è®¤è¯å®‰å…¨")
        if re.search(r'æ”¯ä»˜|é‡‘é¢|äº¤æ˜“', text, re.IGNORECASE):
            risks.append("æ”¯ä»˜å®‰å…¨")
        if re.search(r'ä¸Šä¼ |æ–‡ä»¶|é™„ä»¶', text, re.IGNORECASE):
            risks.append("æ–‡ä»¶ä¸Šä¼ å®‰å…¨")
        
        # æ€§èƒ½é£é™©
        if re.search(r'å¤§é‡|æ‰¹é‡|å¹¶å‘|é«˜é¢‘', text, re.IGNORECASE):
            risks.append("é«˜å¹¶å‘æ€§èƒ½")
        if re.search(r'æŸ¥è¯¢|æœç´¢|æ£€ç´¢', text, re.IGNORECASE):
            risks.append("æŸ¥è¯¢æ€§èƒ½")
        
        # æ•°æ®é£é™©
        if re.search(r'æ•°æ®åº“|å­˜å‚¨|å¤‡ä»½', text, re.IGNORECASE):
            risks.append("æ•°æ®ä¸€è‡´æ€§")
        if re.search(r'åŒæ­¥|å¼‚æ­¥|é˜Ÿåˆ—', text, re.IGNORECASE):
            risks.append("æ•°æ®åŒæ­¥")
        
        return risks
    
    def _extract_critical_paths(self, text: str) -> List[str]:
        """æå–å…³é”®è·¯å¾„"""
        paths = []
        
        # æŸ¥æ‰¾æµç¨‹æè¿°
        flow_patterns = [
            r'æµç¨‹[ï¼š:]\s*([^\n]+)',
            r'æ­¥éª¤[ï¼š:]\s*([^\n]+)',
            r'\d+[\.ã€]\s*([^\n]+)'
        ]
        
        for pattern in flow_patterns:
            matches = re.findall(pattern, text)
            paths.extend(matches)
        
        # è¯†åˆ«å…³é”®ä¸šåŠ¡è·¯å¾„
        if re.search(r'æ³¨å†Œ|ç™»å½•', text, re.IGNORECASE):
            paths.append("ç”¨æˆ·æ³¨å†Œç™»å½•è·¯å¾„")
        if re.search(r'ä¸‹å•|æ”¯ä»˜|è´­ä¹°', text, re.IGNORECASE):
            paths.append("è®¢å•æ”¯ä»˜è·¯å¾„")
        if re.search(r'å®¡æ ¸|å®¡æ‰¹|æµè½¬', text, re.IGNORECASE):
            paths.append("å®¡æ‰¹æµè½¬è·¯å¾„")
        
        return list(set(paths))
    
    def _analyze_data_patterns(self, text: str) -> List[Dict]:
        """åˆ†ææ•°æ®æ¨¡å¼"""
        patterns = []
        
        # è¯†åˆ«è¾“å…¥æ¨¡å¼
        for pattern_name, regex in self.pattern_library["input_patterns"].items():
            if pattern_name in text.lower():
                patterns.append({
                    "type": "input_validation",
                    "name": pattern_name,
                    "pattern": regex,
                    "risk_level": "medium"
                })
        
        # è¯†åˆ«æ•°æ®ç±»å‹
        if re.search(r'æ•°å­—|é‡‘é¢|ä»·æ ¼|æ•°é‡', text, re.IGNORECASE):
            patterns.append({
                "type": "numeric",
                "name": "æ•°å€¼ç±»å‹",
                "validation": "èŒƒå›´æ£€æŸ¥",
                "risk_level": "high"
            })
        
        if re.search(r'æ—¥æœŸ|æ—¶é—´', text, re.IGNORECASE):
            patterns.append({
                "type": "datetime",
                "name": "æ—¥æœŸæ—¶é—´",
                "validation": "æ ¼å¼å’ŒèŒƒå›´æ£€æŸ¥",
                "risk_level": "medium"
            })
        
        return patterns
    
    def _extract_business_rules(self, text: str) -> List[Dict]:
        """æå–ä¸šåŠ¡è§„åˆ™"""
        rules = []
        
        # æ¡ä»¶è§„åˆ™
        condition_patterns = [
            r'å¦‚æœ(.+?)é‚£ä¹ˆ(.+?)(?:\n|$)',
            r'å½“(.+?)æ—¶(.+?)(?:\n|$)',
            r'è‹¥(.+?)åˆ™(.+?)(?:\n|$)'
        ]
        
        for pattern in condition_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for condition, action in matches:
                rules.append({
                    "type": "conditional",
                    "condition": condition.strip(),
                    "action": action.strip(),
                    "complexity": "medium"
                })
        
        # çº¦æŸè§„åˆ™
        constraint_patterns = [
            r'å¿…é¡»(.+?)(?:\n|$)',
            r'ä¸èƒ½(.+?)(?:\n|$)',
            r'é™åˆ¶(.+?)(?:\n|$)'
        ]
        
        for pattern in constraint_patterns:
            matches = re.findall(pattern, text)
            for constraint in matches:
                rules.append({
                    "type": "constraint",
                    "description": constraint.strip(),
                    "complexity": "low"
                })
        
        return rules
    
    def _identify_integration_points(self, text: str) -> List[str]:
        """è¯†åˆ«é›†æˆç‚¹"""
        integrations = []
        
        integration_keywords = [
            "API", "æ¥å£", "ç¬¬ä¸‰æ–¹", "å¤–éƒ¨ç³»ç»Ÿ", "å¾®æœåŠ¡",
            "æ•°æ®åº“", "ç¼“å­˜", "æ¶ˆæ¯é˜Ÿåˆ—", "æ–‡ä»¶ç³»ç»Ÿ"
        ]
        
        for keyword in integration_keywords:
            if keyword in text:
                integrations.append(f"{keyword}é›†æˆ")
        
        return integrations
    
    def _identify_performance_concerns(self, text: str) -> List[str]:
        """è¯†åˆ«æ€§èƒ½å…³æ³¨ç‚¹"""
        concerns = []
        
        if re.search(r'å“åº”æ—¶é—´|å»¶è¿Ÿ|é€Ÿåº¦', text, re.IGNORECASE):
            concerns.append("å“åº”æ—¶é—´æ€§èƒ½")
        if re.search(r'å¹¶å‘|åŒæ—¶|æ‰¹é‡', text, re.IGNORECASE):
            concerns.append("å¹¶å‘å¤„ç†æ€§èƒ½")
        if re.search(r'å¤§æ•°æ®|æµ·é‡|TB|GB', text, re.IGNORECASE):
            concerns.append("å¤§æ•°æ®å¤„ç†æ€§èƒ½")
        if re.search(r'å†…å­˜|CPU|ç£ç›˜', text, re.IGNORECASE):
            concerns.append("èµ„æºä½¿ç”¨æ€§èƒ½")
        
        return concerns
    
    def _identify_security_risks(self, text: str) -> List[str]:
        """è¯†åˆ«å®‰å…¨é£é™©"""
        risks = []
        
        security_keywords = {
            "æ³¨å…¥æ”»å‡»": ["è¾“å…¥", "æŸ¥è¯¢", "SQL"],
            "è·¨ç«™è„šæœ¬": ["è¾“å‡º", "æ˜¾ç¤º", "HTML"],
            "æƒé™æ§åˆ¶": ["æƒé™", "è§’è‰²", "è®¿é—®"],
            "æ•°æ®æ³„éœ²": ["æ•æ„Ÿ", "éšç§", "åŠ å¯†"],
            "ä¼šè¯å®‰å…¨": ["ç™»å½•", "ä¼šè¯", "token"]
        }
        
        for risk_type, keywords in security_keywords.items():
            if any(keyword in text for keyword in keywords):
                risks.append(risk_type)
        
        return risks
    
    def _identify_usability_factors(self, text: str) -> List[str]:
        """è¯†åˆ«å¯ç”¨æ€§å› ç´ """
        factors = []
        
        if re.search(r'ç•Œé¢|UI|ç”¨æˆ·ä½“éªŒ', text, re.IGNORECASE):
            factors.append("ç•Œé¢å‹å¥½æ€§")
        if re.search(r'æç¤º|å¸®åŠ©|å¼•å¯¼', text, re.IGNORECASE):
            factors.append("æ“ä½œå¼•å¯¼")
        if re.search(r'é”™è¯¯|å¼‚å¸¸|å¤±è´¥', text, re.IGNORECASE):
            factors.append("é”™è¯¯å¤„ç†")
        if re.search(r'ç§»åŠ¨|æ‰‹æœº|å“åº”å¼', text, re.IGNORECASE):
            factors.append("ç§»åŠ¨ç«¯é€‚é…")
        
        return factors

    def generate_ai_enhanced_test_cases(self, requirement_text: str,
                                      historical_defects: List[str] = None) -> List[TestCase]:
        """AIå¢å¼ºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ"""
        # é¦–å…ˆè¿›è¡ŒAIåˆ†æ
        ai_analysis = self.ai_analyze_requirements(requirement_text)

        # åŸºç¡€åˆ†æ
        basic_analysis = self.analyze_requirements(requirement_text)

        # æå–ä¸šåŠ¡æ¨¡å—ä¿¡æ¯
        business_modules = self._extract_business_modules(requirement_text)

        # ç”ŸæˆåŸºç¡€æµ‹è¯•ç”¨ä¾‹
        basic_cases = super().generate_all_test_cases(requirement_text, historical_defects)

        # AIå¢å¼ºæµ‹è¯•ç”¨ä¾‹
        ai_cases = []

        # 1. æ™ºèƒ½è¾¹ç•Œåˆ†æ
        ai_cases.extend(self._generate_intelligent_boundary_cases(ai_analysis, basic_analysis, business_modules))

        # 2. è¯­ä¹‰ç­‰ä»·ç±»åˆ’åˆ†
        ai_cases.extend(self._generate_semantic_equivalence_cases(ai_analysis, basic_analysis, business_modules))

        # 3. ä¸Šä¸‹æ–‡åœºæ™¯åˆ†æ
        ai_cases.extend(self._generate_context_scenario_cases(ai_analysis, basic_analysis, business_modules))

        # 4. é¢„æµ‹æ€§é”™è¯¯åˆ†æ
        ai_cases.extend(self._generate_predictive_error_cases(ai_analysis, basic_analysis, business_modules))

        # 5. è‡ªé€‚åº”ç»„åˆæµ‹è¯•
        ai_cases.extend(self._generate_adaptive_combination_cases(ai_analysis, basic_analysis, business_modules))

        # 6. åŸºäºé£é™©çš„ä¼˜å…ˆçº§è°ƒæ•´
        self._adjust_risk_based_priority(basic_cases + ai_cases, ai_analysis)

        # 7. è¦†ç›–åº¦ä¼˜åŒ–
        self._optimize_coverage(basic_cases + ai_cases, ai_analysis)

        # 8. æ™ºèƒ½å›å½’æµ‹è¯•
        regression_cases = self._generate_intelligent_regression_cases(ai_analysis, historical_defects, business_modules)

        # åˆå¹¶æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        all_cases = basic_cases + ai_cases + regression_cases

        # å»é‡å’Œä¼˜åŒ–
        final_cases = self._deduplicate_and_optimize(all_cases)

        self.test_cases = final_cases
        return final_cases

    def _extract_business_modules(self, requirement_text: str) -> Dict[str, str]:
        """ä»éœ€æ±‚æ–‡æœ¬ä¸­æå–ä¸šåŠ¡æ¨¡å—ä¿¡æ¯"""
        text_lower = requirement_text.lower()

        # ä¸šåŠ¡æ¨¡å—å…³é”®è¯æ˜ å°„
        module_mapping = {
            # ç”¨æˆ·ç›¸å…³
            'ç™»å½•': ('ç”¨æˆ·ç®¡ç†', 'ç”¨æˆ·ç™»å½•'),
            'æ³¨å†Œ': ('ç”¨æˆ·ç®¡ç†', 'ç”¨æˆ·æ³¨å†Œ'),
            'ä¸ªäººä¿¡æ¯': ('ä¸ªäººä¸­å¿ƒ', 'åŸºæœ¬ä¿¡æ¯'),
            'ä¸ªäººä¸­å¿ƒ': ('ä¸ªäººä¸­å¿ƒ', 'ä¸ªäººèµ„æ–™'),
            'è®¾ç½®': ('ä¸ªäººä¸­å¿ƒ', 'è´¦æˆ·è®¾ç½®'),
            'å¯†ç ': ('ç”¨æˆ·ç®¡ç†', 'å¯†ç ç®¡ç†'),

            # é¡µé¢ç›¸å…³
            'é¦–é¡µ': ('é¦–é¡µ', 'é¡µé¢å±•ç¤º'),
            'å¯¼èˆª': ('é¦–é¡µ', 'å¯¼èˆªæ '),
            'è½®æ’­': ('é¦–é¡µ', 'è½®æ’­å›¾'),
            'èœå•': ('é¦–é¡µ', 'èœå•æ '),

            # å•†å“ç›¸å…³
            'å•†å“': ('å•†å“ç®¡ç†', 'å•†å“å±•ç¤º'),
            'åˆ†ç±»': ('å•†å“ç®¡ç†', 'åˆ†ç±»ç®¡ç†'),
            'æœç´¢': ('æœç´¢åŠŸèƒ½', 'å•†å“æœç´¢'),
            'ç­›é€‰': ('æœç´¢åŠŸèƒ½', 'æ¡ä»¶ç­›é€‰'),
            'è¯¦æƒ…': ('å•†å“ç®¡ç†', 'å•†å“è¯¦æƒ…'),

            # è´­ç‰©ç›¸å…³
            'è´­ç‰©è½¦': ('è´­ç‰©è½¦ç®¡ç†', 'è´­ç‰©è½¦æ“ä½œ'),
            'è®¢å•': ('è®¢å•ç®¡ç†', 'è®¢å•å¤„ç†'),
            'æ”¯ä»˜': ('æ”¯ä»˜ç®¡ç†', 'æ”¯ä»˜å¤„ç†'),
            'ç»“ç®—': ('è®¢å•ç®¡ç†', 'è®¢å•ç»“ç®—'),

            # åº“å­˜ç›¸å…³
            'åº“å­˜': ('åº“å­˜ç®¡ç†', 'åº“å­˜æ§åˆ¶'),
            'å…¥åº“': ('åº“å­˜ç®¡ç†', 'å•†å“å…¥åº“'),
            'å‡ºåº“': ('åº“å­˜ç®¡ç†', 'å•†å“å‡ºåº“'),

            # å…¶ä»–åŠŸèƒ½
            'è¯„ä»·': ('è¯„ä»·ç®¡ç†', 'å•†å“è¯„ä»·'),
            'æ”¶è—': ('æ”¶è—ç®¡ç†', 'å•†å“æ”¶è—'),
            'åœ°å€': ('åœ°å€ç®¡ç†', 'æ”¶è´§åœ°å€'),
            'ä¼˜æƒ åˆ¸': ('è¥é”€ç®¡ç†', 'ä¼˜æƒ åˆ¸'),
            'ç§¯åˆ†': ('ç§¯åˆ†ç®¡ç†', 'ç§¯åˆ†æ“ä½œ'),
            'å®¢æœ': ('å®¢æœç³»ç»Ÿ', 'åœ¨çº¿å®¢æœ'),
            'æ¶ˆæ¯': ('æ¶ˆæ¯ä¸­å¿ƒ', 'æ¶ˆæ¯é€šçŸ¥'),
            'åé¦ˆ': ('æ„è§åé¦ˆ', 'ç”¨æˆ·åé¦ˆ'),
            'å¸®åŠ©': ('å¸®åŠ©ä¸­å¿ƒ', 'ä½¿ç”¨å¸®åŠ©'),
            'å…³äº': ('ç³»ç»Ÿä¿¡æ¯', 'å…³äºæˆ‘ä»¬')
        }

        # æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å—
        detected_modules = {}
        for keyword, (module, submodule) in module_mapping.items():
            if keyword in text_lower:
                detected_modules[keyword] = {'module': module, 'submodule': submodule}

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç‰¹å®šæ¨¡å—ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å—
        if not detected_modules:
            detected_modules['default'] = {'module': 'åŠŸèƒ½æ¨¡å—', 'submodule': 'åŸºç¡€åŠŸèƒ½'}

        return detected_modules

    def _generate_intelligent_boundary_cases(self, ai_analysis: AIAnalysisResult,
                                           basic_analysis: RequirementAnalysis,
                                           business_modules: Dict[str, str]) -> List[TestCase]:
        """æ™ºèƒ½è¾¹ç•Œåˆ†ææµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # è·å–ä¸»è¦ä¸šåŠ¡æ¨¡å—
        main_module = list(business_modules.values())[0] if business_modules else {'module': 'åŠŸèƒ½æ¨¡å—', 'submodule': 'åŸºç¡€åŠŸèƒ½'}

        for pattern in ai_analysis.data_patterns:
            if pattern["type"] == "numeric":
                # åŸºäºå¤æ‚åº¦åŠ¨æ€è°ƒæ•´è¾¹ç•Œå€¼
                if ai_analysis.complexity_score > 0.7:
                    # é«˜å¤æ‚åº¦ï¼šæ›´å¤šè¾¹ç•Œå€¼
                    boundary_values = [-1, 0, 1, 999, 1000]
                else:
                    # ä½å¤æ‚åº¦ï¼šæ ‡å‡†è¾¹ç•Œå€¼
                    boundary_values = [-1, 0, 1, 100]

                for value in boundary_values[:3]:  # é™åˆ¶æ•°é‡
                    submodule = f"{pattern['name']}è¾¹ç•Œæµ‹è¯•"
                    case_id = self.generate_case_id(main_module['module'], submodule)
                    priority = Priority.P0 if value in [0, -1, 1] else Priority.P1

                    test_case = TestCase(
                        module=main_module['module'],
                        submodule=submodule,
                        case_id=case_id,
                        title=f"éªŒè¯{f"{pattern['name']}è¾¹ç•Œæµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œç”¨æˆ·å·²ç™»å½•",
                        test_steps=f"1. åœ¨{pattern['name']}å­—æ®µè¾“å…¥è¾¹ç•Œå€¼{value}\n2. ç‚¹å‡»æäº¤æŒ‰é’®\n3. éªŒè¯ç³»ç»Ÿå“åº”\n4. æ£€æŸ¥é”™è¯¯æç¤ºä¿¡æ¯",
                        expected=f"ç³»ç»Ÿæ­£ç¡®å¤„ç†è¾¹ç•Œå€¼{value}ï¼Œæ˜¾ç¤ºç›¸åº”çš„æç¤ºä¿¡æ¯",
                        priority=priority,
                        remark=f"è¾¹ç•Œå€¼åˆ†æ - {pattern['name']}å­—æ®µè¾¹ç•Œæµ‹è¯•",
                        methods_used=[TestMethod.BOUNDARY_VALUE]
                    
                    )
                    cases.append(test_case)

        return cases

    def _generate_semantic_equivalence_cases(self, ai_analysis: AIAnalysisResult,
                                           basic_analysis: RequirementAnalysis,
                                           business_modules: Dict[str, str]) -> List[TestCase]:
        """è¯­ä¹‰ç­‰ä»·ç±»åˆ’åˆ†æµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # è·å–ä¸»è¦ä¸šåŠ¡æ¨¡å—
        main_module = list(business_modules.values())[0] if business_modules else {'module': 'æ•°æ®éªŒè¯', 'submodule': 'ç­‰ä»·ç±»æµ‹è¯•'}

        # åŸºäºä¸šåŠ¡è§„åˆ™ç”Ÿæˆç­‰ä»·ç±»æµ‹è¯•
        for rule in ai_analysis.business_rules[:2]:  # é™åˆ¶æ•°é‡
            if rule["type"] == "conditional":
                # æ­£å‘ç­‰ä»·ç±»
                submodule = "æ¡ä»¶æ»¡è¶³æµ‹è¯•"
                case_id = self.generate_case_id(main_module['module'], submodule)
                test_case = TestCase(
                        module=main_module['module'],
                        submodule=submodule,
                        case_id=case_id,
                        title=f"éªŒè¯{"æ¡ä»¶æ»¡è¶³æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œç”¨æˆ·å·²ç™»å½•",
                        test_steps=f"1. è®¾ç½®æ¡ä»¶ï¼š{rule['condition']}\n2. æ‰§è¡Œç›¸å…³æ“ä½œ\n3. éªŒè¯ä¸šåŠ¡ç»“æœ\n4. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€",
                        expected=f"æ»¡è¶³æ¡ä»¶æ—¶æ­£ç¡®æ‰§è¡Œä¸šåŠ¡é€»è¾‘",
                        priority=Priority.P0,
                        remark=f"ç­‰ä»·ç±»åˆ†æ - æ¡ä»¶æ»¡è¶³æ—¶çš„ä¸šåŠ¡éªŒè¯",
                        methods_used=[TestMethod.EQUIVALENCE]
                
                    )
                cases.append(test_case)

                # åå‘ç­‰ä»·ç±»
                submodule = "æ¡ä»¶ä¸æ»¡è¶³æµ‹è¯•"
                case_id = self.generate_case_id(main_module['module'], submodule)
                test_case = TestCase(
                        module=main_module['module'],
                        submodule=submodule,
                        case_id=case_id,
                        title=f"éªŒè¯{"æ¡ä»¶ä¸æ»¡è¶³æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œç”¨æˆ·å·²ç™»å½•",
                        test_steps=f"1. è®¾ç½®æ¡ä»¶ä¸æ»¡è¶³ï¼š{rule['condition']}\n2. å°è¯•æ‰§è¡Œæ“ä½œ\n3. éªŒè¯é”™è¯¯å¤„ç†\n4. æ£€æŸ¥æç¤ºä¿¡æ¯",
                        expected=f"æ¡ä»¶ä¸æ»¡è¶³æ—¶æ­£ç¡®å¤„ç†ï¼Œæ˜¾ç¤ºå‹å¥½æç¤º",
                        priority=Priority.P1,
                        remark=f"ç­‰ä»·ç±»åˆ†æ - æ¡ä»¶ä¸æ»¡è¶³æ—¶çš„å¤„ç†éªŒè¯",
                        methods_used=[TestMethod.EQUIVALENCE]
                
                    )
                cases.append(test_case)

        return cases

    def _generate_context_scenario_cases(self, ai_analysis: AIAnalysisResult,
                                       basic_analysis: RequirementAnalysis,
                                       business_modules: Dict[str, str]) -> List[TestCase]:
        """ä¸Šä¸‹æ–‡åœºæ™¯åˆ†ææµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # è·å–ä¸»è¦ä¸šåŠ¡æ¨¡å—
        main_module = list(business_modules.values())[0] if business_modules else {'module': 'ä¸šåŠ¡æµç¨‹', 'submodule': 'åœºæ™¯æµ‹è¯•'}

        # åŸºäºå…³é”®è·¯å¾„ç”Ÿæˆä¸Šä¸‹æ–‡åœºæ™¯
        for path in ai_analysis.critical_paths[:2]:  # é™åˆ¶æ•°é‡
            # æ­£å¸¸åœºæ™¯
            case_id = self.generate_case_id(main_module['module'])
            test_case = TestCase(
                        module=main_module['module'],
                        submodule="æ­£å¸¸æµç¨‹æµ‹è¯•",
                        case_id=case_id,
                        title=f"éªŒè¯{"æ­£å¸¸æµç¨‹æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œç”¨æˆ·å·²ç™»å½•",
                        test_steps=f"1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒ\n2. æ‰§è¡Œä¸šåŠ¡æµç¨‹ï¼š{path}\n3. éªŒè¯æµç¨‹ç»“æœ\n4. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§",
                        expected="ä¸šåŠ¡æµç¨‹æ­£å¸¸æ‰§è¡Œï¼Œç»“æœç¬¦åˆé¢„æœŸ",
                        priority=Priority.P0,
                        remark=f"åœºæ™¯æµ‹è¯• - æ­£å¸¸ä¸šåŠ¡æµç¨‹éªŒè¯",
                        methods_used=[TestMethod.SCENARIO]
            
                    )
            cases.append(test_case)

            # å¼‚å¸¸åœºæ™¯
            case_id = self.generate_case_id(main_module['module'])
            test_case = TestCase(
                        module=main_module['module'],
                        submodule="å¼‚å¸¸æµç¨‹æµ‹è¯•",
                        case_id=case_id,
                        title=f"éªŒè¯{"å¼‚å¸¸æµç¨‹æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿè¿è¡Œä¸­ï¼Œå­˜åœ¨å¼‚å¸¸æ¡ä»¶",
                        test_steps=f"1. æ¨¡æ‹Ÿå¼‚å¸¸ç¯å¢ƒ\n2. å°è¯•æ‰§è¡Œï¼š{path}\n3. éªŒè¯é”™è¯¯å¤„ç†\n4. æ£€æŸ¥ç³»ç»Ÿæ¢å¤",
                        expected="ç³»ç»Ÿæ­£ç¡®å¤„ç†å¼‚å¸¸ï¼Œæ˜¾ç¤ºå‹å¥½é”™è¯¯ä¿¡æ¯",
                        priority=Priority.P1,
                        remark=f"åœºæ™¯æµ‹è¯• - å¼‚å¸¸æƒ…å†µå¤„ç†éªŒè¯",
                        methods_used=[TestMethod.SCENARIO]
            
                    )
            cases.append(test_case)

        return cases

    def _generate_predictive_error_cases(self, ai_analysis: AIAnalysisResult,
                                       basic_analysis: RequirementAnalysis,
                                       business_modules: Dict[str, str]) -> List[TestCase]:
        """é¢„æµ‹æ€§é”™è¯¯åˆ†ææµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # åŸºäºé£é™©åŒºåŸŸé¢„æµ‹é”™è¯¯
        for risk in ai_analysis.risk_areas:
            # å®‰å…¨é£é™©æµ‹è¯•
            if "å®‰å…¨" in risk:
                case_id = self.generate_case_id("AIé¢„æµ‹é”™è¯¯")
                test_case = TestCase(
                        module="AIé¢„æµ‹é”™è¯¯",
                        submodule="å®‰å…¨é£é™©é¢„æµ‹",
                        case_id=case_id,
                        title=f"éªŒè¯{"å®‰å…¨é£é™©é¢„æµ‹"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œå…·å¤‡å®‰å…¨æµ‹è¯•ç¯å¢ƒ",
                        test_steps=f"1. æ¨¡æ‹Ÿ{risk}ç›¸å…³çš„æ”»å‡»åœºæ™¯\n2. æ‰§è¡Œæ½œåœ¨çš„æ¶æ„æ“ä½œ\n3. éªŒè¯å®‰å…¨é˜²æŠ¤æœºåˆ¶\n4. æ£€æŸ¥æ—¥å¿—è®°å½•",
                        expected="ç³»ç»Ÿæ­£ç¡®è¯†åˆ«å’Œé˜»æ­¢å®‰å…¨å¨èƒï¼Œè®°å½•å®‰å…¨äº‹ä»¶",
                        priority=Priority.P0,
                        remark=f"{AITestMethod.PREDICTIVE_ERROR.value}ã€‚åŸºäº{risk}çš„é¢„æµ‹æ€§æµ‹è¯•",
                        methods_used=[TestMethod.ERROR_GUESSING]
                
                    )
                cases.append(test_case)

            # æ€§èƒ½é£é™©æµ‹è¯•
            elif "æ€§èƒ½" in risk:
                case_id = self.generate_case_id("AIé¢„æµ‹é”™è¯¯")
                test_case = TestCase(
                        module="AIé¢„æµ‹é”™è¯¯",
                        submodule="æ€§èƒ½é£é™©é¢„æµ‹",
                        case_id=case_id,
                        title=f"éªŒè¯{"æ€§èƒ½é£é™©é¢„æµ‹"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œæ€§èƒ½ç›‘æ§å·²å¯ç”¨",
                        test_steps=f"1. æ¨¡æ‹Ÿ{risk}ç›¸å…³çš„æ€§èƒ½å‹åŠ›\n2. ç›‘æ§ç³»ç»Ÿå“åº”æ—¶é—´\n3. æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ\n4. éªŒè¯æ€§èƒ½é˜ˆå€¼",
                        expected="ç³»ç»Ÿåœ¨å‹åŠ›ä¸‹ä¿æŒç¨³å®šï¼Œæ€§èƒ½æŒ‡æ ‡åœ¨å¯æ¥å—èŒƒå›´å†…",
                        priority=Priority.P1,
                        remark=f"{AITestMethod.PREDICTIVE_ERROR.value}ã€‚åŸºäº{risk}çš„æ€§èƒ½é¢„æµ‹æµ‹è¯•",
                        methods_used=[TestMethod.ERROR_GUESSING]
                
                    )
                cases.append(test_case)

        return cases

    def _generate_adaptive_combination_cases(self, ai_analysis: AIAnalysisResult,
                                           basic_analysis: RequirementAnalysis,
                                           business_modules: Dict[str, str]) -> List[TestCase]:
        """è‡ªé€‚åº”ç»„åˆæµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # åŸºäºå¤æ‚åº¦è‡ªé€‚åº”é€‰æ‹©ç»„åˆç­–ç•¥
        if ai_analysis.complexity_score > 0.8:
            # é«˜å¤æ‚åº¦ï¼šä½¿ç”¨æ›´å…¨é¢çš„ç»„åˆ
            combination_strategy = "å…¨ç»„åˆ"
            combinations = list(itertools.product([True, False], repeat=min(4, len(ai_analysis.business_rules))))
        elif ai_analysis.complexity_score > 0.5:
            # ä¸­ç­‰å¤æ‚åº¦ï¼šä½¿ç”¨æ­£äº¤ç»„åˆ
            combination_strategy = "æ­£äº¤ç»„åˆ"
            combinations = [(True, True, False), (True, False, True), (False, True, True), (False, False, False)]
        else:
            # ä½å¤æ‚åº¦ï¼šä½¿ç”¨åŸºæœ¬ç»„åˆ
            combination_strategy = "åŸºæœ¬ç»„åˆ"
            combinations = [(True, True), (True, False), (False, True), (False, False)]

        for i, combination in enumerate(combinations[:8]):  # é™åˆ¶ç»„åˆæ•°é‡
            case_id = self.generate_case_id("AIè‡ªé€‚åº”ç»„åˆ")

            # æ„å»ºç»„åˆæ¡ä»¶æè¿°
            conditions = []
            for j, value in enumerate(combination):
                if j < len(ai_analysis.business_rules):
                    rule = ai_analysis.business_rules[j]
                    condition_desc = f"{rule.get('condition', f'æ¡ä»¶{j+1}')}={'æ»¡è¶³' if value else 'ä¸æ»¡è¶³'}"
                    conditions.append(condition_desc)

            test_case = TestCase(
                        module="AIè‡ªé€‚åº”ç»„åˆ",
                        submodule=f"{combination_strategy}æµ‹è¯•",
                        case_id=case_id,
                        title=f"éªŒè¯{f"{combination_strategy}æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œæµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡",
                        test_steps=f"1. è®¾ç½®ç»„åˆæ¡ä»¶ï¼š{'; '.join(conditions)}\n2. æ‰§è¡Œä¸šåŠ¡æ“ä½œ\n3. éªŒè¯ç»„åˆç»“æœ\n4. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€ä¸€è‡´æ€§",
                        expected="ç³»ç»Ÿæ­£ç¡®å¤„ç†æ¡ä»¶ç»„åˆï¼Œä¸šåŠ¡é€»è¾‘ç¬¦åˆé¢„æœŸ",
                        priority=Priority.P1,
                        remark=f"{AITestMethod.ADAPTIVE_COMBINATION.value}ã€‚{combination_strategy}ï¼Œå¤æ‚åº¦ï¼š{ai_analysis.complexity_score:.2f}",
                        methods_used=[TestMethod.ORTHOGONAL]
            
                    )
            cases.append(test_case)

        return cases

    def _adjust_risk_based_priority(self, test_cases: List[TestCase], ai_analysis: AIAnalysisResult):
        """åŸºäºé£é™©è°ƒæ•´ä¼˜å…ˆçº§"""
        high_risk_keywords = ["å®‰å…¨", "æ”¯ä»˜", "è®¤è¯", "æƒé™", "æ•°æ®"]

        for case in test_cases:
            # æ£€æŸ¥æ˜¯å¦æ¶‰åŠé«˜é£é™©åŒºåŸŸ
            case_text = f"{case.module} {case.submodule} {case.test_steps} {case.remark}".lower()

            # é«˜é£é™©æå‡ä¼˜å…ˆçº§
            if any(keyword in case_text for keyword in high_risk_keywords):
                if case.priority == Priority.P2:
                    case.priority = Priority.P1
                elif case.priority == Priority.P1:
                    case.priority = Priority.P0

            # åŸºäºAIåˆ†æçš„é£é™©åŒºåŸŸè°ƒæ•´
            for risk in ai_analysis.risk_areas:
                if risk.lower() in case_text:
                    case.priority = Priority.P0
                    case.remark += f"ã€‚é«˜é£é™©åŒºåŸŸï¼š{risk}"
                    break

    def _optimize_coverage(self, test_cases: List[TestCase], ai_analysis: AIAnalysisResult) -> List[TestCase]:
        """è¦†ç›–åº¦ä¼˜åŒ–"""
        # åˆ†æè¦†ç›–åº¦
        coverage_analysis = {
            "modules": set(),
            "methods": set(),
            "risk_areas": set(),
            "business_rules": set()
        }

        for case in test_cases:
            coverage_analysis["modules"].add(case.module)
            coverage_analysis["methods"].update(case.methods_used)

            # æ£€æŸ¥é£é™©åŒºåŸŸè¦†ç›–
            case_text = f"{case.module} {case.submodule} {case.test_steps}".lower()
            for risk in ai_analysis.risk_areas:
                if risk.lower() in case_text:
                    coverage_analysis["risk_areas"].add(risk)

        # ç”Ÿæˆè¡¥å……æµ‹è¯•ç”¨ä¾‹ä»¥æé«˜è¦†ç›–åº¦
        missing_coverage = []

        # æ£€æŸ¥æœªè¦†ç›–çš„é£é™©åŒºåŸŸ
        for risk in ai_analysis.risk_areas:
            if risk not in coverage_analysis["risk_areas"]:
                missing_coverage.append(f"é£é™©åŒºåŸŸï¼š{risk}")

        # æ£€æŸ¥æœªè¦†ç›–çš„é›†æˆç‚¹
        for integration in ai_analysis.integration_points:
            integration_covered = any(integration.lower() in case.test_steps.lower() for case in test_cases)
            if not integration_covered:
                missing_coverage.append(f"é›†æˆç‚¹ï¼š{integration}")

        # ä¸ºæœªè¦†ç›–çš„åŒºåŸŸç”Ÿæˆè¡¥å……æµ‹è¯•ç”¨ä¾‹
        supplementary_cases = []
        for missing in missing_coverage[:5]:  # é™åˆ¶è¡¥å……ç”¨ä¾‹æ•°é‡
            case_id = self.generate_case_id("AIè¦†ç›–åº¦ä¼˜åŒ–")
            test_case = TestCase(
                        module="AIè¦†ç›–åº¦ä¼˜åŒ–",
                        submodule="è¡¥å……è¦†ç›–",
                        case_id=case_id,
                        title=f"éªŒè¯{"è¡¥å……è¦†ç›–"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿæ­£å¸¸è¿è¡Œï¼Œè¦†ç›–åº¦åˆ†æå·²å®Œæˆ",
                        test_steps=f"1. é’ˆå¯¹{missing}è¿›è¡Œä¸“é¡¹æµ‹è¯•\n2. éªŒè¯åŠŸèƒ½å®Œæ•´æ€§\n3. æ£€æŸ¥è¾¹ç•Œæƒ…å†µ\n4. ç¡®è®¤é”™è¯¯å¤„ç†",
                        expected=f"å®Œæ•´è¦†ç›–{missing}ï¼ŒåŠŸèƒ½æ­£å¸¸",
                        priority=Priority.P2,
                        remark=f"{AITestMethod.COVERAGE_OPTIMIZATION.value}ã€‚è¡¥å……è¦†ç›–ï¼š{missing}",
                        methods_used=[TestMethod.ERROR_GUESSING]
            
                    )
            supplementary_cases.append(test_case)

        return test_cases + supplementary_cases

    def _generate_intelligent_regression_cases(self, ai_analysis: AIAnalysisResult,
                                             historical_defects: List[str] = None,
                                             business_modules: Dict[str, str] = None) -> List[TestCase]:
        """æ™ºèƒ½å›å½’æµ‹è¯•ç”¨ä¾‹"""
        cases = []

        if not historical_defects:
            return cases

        # AIåˆ†æå†å²ç¼ºé™·æ¨¡å¼
        defect_patterns = self._analyze_defect_patterns(historical_defects)

        for defect in historical_defects:
            # åŸºç¡€å›å½’æµ‹è¯•
            case_id = self.generate_case_id("AIæ™ºèƒ½å›å½’")
            basic_regression = TestCase(
                        module="AIæ™ºèƒ½å›å½’",
                        submodule="å†å²ç¼ºé™·å›å½’",
                        case_id=case_id,
                        title=f"éªŒè¯{"å†å²ç¼ºé™·å›å½’"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿå·²ä¿®å¤ç›¸å…³ç¼ºé™·ï¼Œæµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡",
                        test_steps=f"1. å¤ç°å†å²ç¼ºé™·åœºæ™¯ï¼š{defect}\n2. éªŒè¯ç¼ºé™·å·²ä¿®å¤\n3. æ£€æŸ¥ä¿®å¤çš„å®Œæ•´æ€§\n4. éªŒè¯æ— æ–°çš„å‰¯ä½œç”¨",
                        expected="å†å²ç¼ºé™·å·²å®Œå…¨ä¿®å¤ï¼Œæ— å›å½’é—®é¢˜",
                        priority=Priority.P0,
                        remark=f"{AITestMethod.INTELLIGENT_REGRESSION.value}ã€‚å†å²ç¼ºé™·ï¼š{defect}",
                        methods_used=[TestMethod.ERROR_GUESSING]
            
                    )
            cases.append(basic_regression)

            # æ‰©å±•å›å½’æµ‹è¯•ï¼ˆåŸºäºAIåˆ†æï¼‰
            if defect_patterns.get("similar_scenarios"):
                case_id = self.generate_case_id("AIæ™ºèƒ½å›å½’")
                extended_regression = TestCase(
                        module="AIæ™ºèƒ½å›å½’",
                        submodule="æ‰©å±•å›å½’æµ‹è¯•",
                        case_id=case_id,
                        title=f"éªŒè¯{"æ‰©å±•å›å½’æµ‹è¯•"}åŠŸèƒ½",
                        precondition="ç³»ç»Ÿå·²ä¿®å¤ç›¸å…³ç¼ºé™·ï¼Œæ‰©å±•æµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡",
                        test_steps=f"1. åŸºäº{defect}åˆ†æç›¸ä¼¼åœºæ™¯\n2. æµ‹è¯•ç›¸å…³åŠŸèƒ½æ¨¡å—\n3. éªŒè¯ä¿®å¤çš„å½±å“èŒƒå›´\n4. æ£€æŸ¥æ½œåœ¨çš„å…³è”é—®é¢˜",
                        expected="ç›¸å…³åŠŸèƒ½æ¨¡å—æ­£å¸¸ï¼Œæ— æ½œåœ¨å›å½’é£é™©",
                        priority=Priority.P1,
                        remark=f"{AITestMethod.INTELLIGENT_REGRESSION.value}ã€‚æ‰©å±•å›å½’ï¼ŒåŸºäºç¼ºé™·æ¨¡å¼åˆ†æ",
                        methods_used=[TestMethod.ERROR_GUESSING]
                
                    )
                cases.append(extended_regression)

        return cases

    def _analyze_defect_patterns(self, historical_defects: List[str]) -> Dict:
        """åˆ†æç¼ºé™·æ¨¡å¼"""
        patterns = {
            "security_related": [],
            "performance_related": [],
            "ui_related": [],
            "data_related": [],
            "similar_scenarios": True
        }

        for defect in historical_defects:
            defect_lower = defect.lower()
            if any(keyword in defect_lower for keyword in ["å®‰å…¨", "æƒé™", "æ³¨å…¥", "xss"]):
                patterns["security_related"].append(defect)
            elif any(keyword in defect_lower for keyword in ["æ€§èƒ½", "æ…¢", "è¶…æ—¶", "å†…å­˜"]):
                patterns["performance_related"].append(defect)
            elif any(keyword in defect_lower for keyword in ["ç•Œé¢", "æ˜¾ç¤º", "ui", "é¡µé¢"]):
                patterns["ui_related"].append(defect)
            elif any(keyword in defect_lower for keyword in ["æ•°æ®", "åº“", "åŒæ­¥", "ä¸€è‡´"]):
                patterns["data_related"].append(defect)

        return patterns

    def _deduplicate_and_optimize(self, test_cases: List[TestCase]) -> List[TestCase]:
        """å»é‡å’Œä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹"""
        # åŸºäºæµ‹è¯•æ­¥éª¤çš„ç›¸ä¼¼åº¦å»é‡
        unique_cases = []
        seen_signatures = set()

        for case in test_cases:
            # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ç­¾å
            signature = f"{case.module}_{case.submodule}_{hash(case.test_steps) % 10000}"

            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_cases.append(case)
            else:
                # å¦‚æœé‡å¤ï¼Œä¿ç•™ä¼˜å…ˆçº§æ›´é«˜çš„
                existing_case = next(c for c in unique_cases if f"{c.module}_{c.submodule}_{hash(c.test_steps) % 10000}" == signature)
                if case.priority.value < existing_case.priority.value:  # P0 < P1 < P2
                    unique_cases.remove(existing_case)
                    unique_cases.append(case)

        # æŒ‰ä¼˜å…ˆçº§å’Œæ¨¡å—æ’åº
        unique_cases.sort(key=lambda x: (x.priority.value, x.module, x.submodule))

        return unique_cases

    def generate_ai_analysis_report(self) -> str:
        """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
        if not self.ai_analysis:
            return "AIåˆ†ææœªå®Œæˆ"

        report = "\n## ğŸ¤– AIæ™ºèƒ½åˆ†ææŠ¥å‘Š\n\n"

        # å¤æ‚åº¦åˆ†æ
        report += f"### ğŸ“Š éœ€æ±‚å¤æ‚åº¦åˆ†æ\n"
        report += f"**å¤æ‚åº¦è¯„åˆ†**: {self.ai_analysis.complexity_score:.2f}/1.0\n"

        complexity_level = "ä½"
        if self.ai_analysis.complexity_score > 0.7:
            complexity_level = "é«˜"
        elif self.ai_analysis.complexity_score > 0.4:
            complexity_level = "ä¸­"

        report += f"**å¤æ‚åº¦ç­‰çº§**: {complexity_level}\n\n"

        # é£é™©åˆ†æ
        if self.ai_analysis.risk_areas:
            report += f"### âš ï¸ é£é™©åŒºåŸŸè¯†åˆ«\n"
            for i, risk in enumerate(self.ai_analysis.risk_areas, 1):
                report += f"{i}. **{risk}** - éœ€è¦é‡ç‚¹å…³æ³¨\n"
            report += "\n"

        # å…³é”®è·¯å¾„
        if self.ai_analysis.critical_paths:
            report += f"### ğŸ›¤ï¸ å…³é”®è·¯å¾„åˆ†æ\n"
            for i, path in enumerate(self.ai_analysis.critical_paths, 1):
                report += f"{i}. {path}\n"
            report += "\n"

        # æ•°æ®æ¨¡å¼
        if self.ai_analysis.data_patterns:
            report += f"### ğŸ“‹ æ•°æ®æ¨¡å¼åˆ†æ\n"
            for pattern in self.ai_analysis.data_patterns:
                report += f"- **{pattern['name']}** ({pattern['type']}) - é£é™©ç­‰çº§: {pattern.get('risk_level', 'unknown')}\n"
            report += "\n"

        # ä¸šåŠ¡è§„åˆ™
        if self.ai_analysis.business_rules:
            report += f"### ğŸ“œ ä¸šåŠ¡è§„åˆ™æå–\n"
            for i, rule in enumerate(self.ai_analysis.business_rules, 1):
                if rule['type'] == 'conditional':
                    report += f"{i}. **æ¡ä»¶è§„åˆ™**: å¦‚æœ {rule['condition']} é‚£ä¹ˆ {rule['action']}\n"
                else:
                    report += f"{i}. **çº¦æŸè§„åˆ™**: {rule['description']}\n"
            report += "\n"

        # é›†æˆç‚¹
        if self.ai_analysis.integration_points:
            report += f"### ğŸ”— é›†æˆç‚¹è¯†åˆ«\n"
            for i, integration in enumerate(self.ai_analysis.integration_points, 1):
                report += f"{i}. {integration}\n"
            report += "\n"

        # æ€§èƒ½å…³æ³¨ç‚¹
        if self.ai_analysis.performance_concerns:
            report += f"### âš¡ æ€§èƒ½å…³æ³¨ç‚¹\n"
            for i, concern in enumerate(self.ai_analysis.performance_concerns, 1):
                report += f"{i}. {concern}\n"
            report += "\n"

        # å®‰å…¨é£é™©
        if self.ai_analysis.security_risks:
            report += f"### ğŸ”’ å®‰å…¨é£é™©è¯„ä¼°\n"
            for i, risk in enumerate(self.ai_analysis.security_risks, 1):
                report += f"{i}. {risk}\n"
            report += "\n"

        # å¯ç”¨æ€§å› ç´ 
        if self.ai_analysis.usability_factors:
            report += f"### ğŸ‘¥ å¯ç”¨æ€§å› ç´ \n"
            for i, factor in enumerate(self.ai_analysis.usability_factors, 1):
                report += f"{i}. {factor}\n"
            report += "\n"

        return report

    def generate_ai_enhanced_statistics(self) -> str:
        """ç”ŸæˆAIå¢å¼ºç»Ÿè®¡æŠ¥å‘Š"""
        if not self.test_cases:
            return "æš‚æ— æµ‹è¯•ç”¨ä¾‹æ•°æ®"

        # åŸºç¡€ç»Ÿè®¡
        basic_stats = self.generate_statistics_report()

        # AIå¢å¼ºç»Ÿè®¡
        ai_stats = "\n## ğŸ¤– AIå¢å¼ºæµ‹è¯•ç»Ÿè®¡\n\n"

        # AIæ–¹æ³•ç»Ÿè®¡
        ai_methods = {}
        for case in self.test_cases:
            if any(ai_method.value in case.remark for ai_method in AITestMethod):
                for ai_method in AITestMethod:
                    if ai_method.value in case.remark:
                        ai_methods[ai_method.value] = ai_methods.get(ai_method.value, 0) + 1

        if ai_methods:
            ai_stats += "### ğŸ§  AIæµ‹è¯•æ–¹æ³•åº”ç”¨\n"
            total_ai_cases = sum(ai_methods.values())
            for method, count in sorted(ai_methods.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(self.test_cases) * 100)
                ai_stats += f"- **{method}**: {count} ä¸ªç”¨ä¾‹ ({percentage:.1f}%)\n"
            ai_stats += f"\n**AIå¢å¼ºç”¨ä¾‹å æ¯”**: {total_ai_cases}/{len(self.test_cases)} ({total_ai_cases/len(self.test_cases)*100:.1f}%)\n\n"

        # å¤æ‚åº¦åˆ†å¸ƒ
        if self.ai_analysis:
            ai_stats += "### ğŸ“Š å¤æ‚åº¦é©±åŠ¨çš„æµ‹è¯•åˆ†å¸ƒ\n"
            complexity_score = self.ai_analysis.complexity_score

            if complexity_score > 0.7:
                ai_stats += f"- **é«˜å¤æ‚åº¦ç³»ç»Ÿ** (è¯„åˆ†: {complexity_score:.2f})\n"
                ai_stats += "  - é‡‡ç”¨å…¨é¢çš„è¾¹ç•Œå€¼æµ‹è¯•\n"
                ai_stats += "  - å¢å¼ºçš„ç»„åˆæµ‹è¯•è¦†ç›–\n"
                ai_stats += "  - æ·±åº¦çš„é”™è¯¯åœºæ™¯åˆ†æ\n"
            elif complexity_score > 0.4:
                ai_stats += f"- **ä¸­ç­‰å¤æ‚åº¦ç³»ç»Ÿ** (è¯„åˆ†: {complexity_score:.2f})\n"
                ai_stats += "  - æ ‡å‡†çš„æµ‹è¯•æ–¹æ³•ç»„åˆ\n"
                ai_stats += "  - é‡ç‚¹å…³æ³¨å…³é”®è·¯å¾„\n"
            else:
                ai_stats += f"- **ä½å¤æ‚åº¦ç³»ç»Ÿ** (è¯„åˆ†: {complexity_score:.2f})\n"
                ai_stats += "  - åŸºç¡€æµ‹è¯•æ–¹æ³•è¦†ç›–\n"
                ai_stats += "  - é‡ç‚¹éªŒè¯æ ¸å¿ƒåŠŸèƒ½\n"
            ai_stats += "\n"

        # é£é™©è¦†ç›–åˆ†æ
        if self.ai_analysis and self.ai_analysis.risk_areas:
            ai_stats += "### âš ï¸ é£é™©è¦†ç›–åˆ†æ\n"
            covered_risks = set()
            for case in self.test_cases:
                case_text = f"{case.module} {case.submodule} {case.test_steps} {case.remark}".lower()
                for risk in self.ai_analysis.risk_areas:
                    if risk.lower() in case_text:
                        covered_risks.add(risk)

            coverage_rate = len(covered_risks) / len(self.ai_analysis.risk_areas) * 100
            ai_stats += f"**é£é™©è¦†ç›–ç‡**: {len(covered_risks)}/{len(self.ai_analysis.risk_areas)} ({coverage_rate:.1f}%)\n\n"

            ai_stats += "**å·²è¦†ç›–é£é™©**:\n"
            for risk in covered_risks:
                ai_stats += f"- âœ… {risk}\n"

            uncovered_risks = set(self.ai_analysis.risk_areas) - covered_risks
            if uncovered_risks:
                ai_stats += "\n**æœªè¦†ç›–é£é™©**:\n"
                for risk in uncovered_risks:
                    ai_stats += f"- âŒ {risk}\n"
            ai_stats += "\n"

        # æ™ºèƒ½ä¼˜åŒ–å»ºè®®
        ai_stats += "### ğŸ’¡ AIä¼˜åŒ–å»ºè®®\n"

        # åŸºäºä¼˜å…ˆçº§åˆ†å¸ƒçš„å»ºè®®
        priority_stats = {}
        for case in self.test_cases:
            priority = case.priority.value
            priority_stats[priority] = priority_stats.get(priority, 0) + 1

        p0_ratio = priority_stats.get('P0', 0) / len(self.test_cases)
        if p0_ratio > 0.4:
            ai_stats += "- âš ï¸ **P0ç”¨ä¾‹æ¯”ä¾‹è¾ƒé«˜** - å»ºè®®é‡æ–°è¯„ä¼°ä¼˜å…ˆçº§åˆ†é…\n"
        elif p0_ratio < 0.1:
            ai_stats += "- âš ï¸ **P0ç”¨ä¾‹æ¯”ä¾‹è¾ƒä½** - å»ºè®®å¢åŠ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•\n"
        else:
            ai_stats += "- âœ… **ä¼˜å…ˆçº§åˆ†é…åˆç†** - P0ç”¨ä¾‹æ¯”ä¾‹é€‚ä¸­\n"

        # åŸºäºå¤æ‚åº¦çš„å»ºè®®
        if self.ai_analysis:
            if self.ai_analysis.complexity_score > 0.8:
                ai_stats += "- ğŸ” **é«˜å¤æ‚åº¦ç³»ç»Ÿ** - å»ºè®®å¢åŠ é›†æˆæµ‹è¯•å’Œç«¯åˆ°ç«¯æµ‹è¯•\n"
            if len(self.ai_analysis.security_risks) > 3:
                ai_stats += "- ğŸ”’ **å®‰å…¨é£é™©è¾ƒå¤š** - å»ºè®®è¿›è¡Œä¸“é¡¹å®‰å…¨æµ‹è¯•\n"
            if len(self.ai_analysis.performance_concerns) > 2:
                ai_stats += "- âš¡ **æ€§èƒ½å…³æ³¨ç‚¹è¾ƒå¤š** - å»ºè®®è¿›è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•\n"

        return basic_stats + ai_stats

    def export_ai_enhanced_report(self, filename: str = None) -> str:
        """å¯¼å‡ºAIå¢å¼ºæŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ai_enhanced_test_report_{timestamp}.md"

        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report_content = "# ğŸ¤– AIå¢å¼ºåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹æŠ¥å‘Š\n\n"
        report_content += f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report_content += f"**æ€»æµ‹è¯•ç”¨ä¾‹æ•°**: {len(self.test_cases)}\n\n"

        # AIåˆ†ææŠ¥å‘Š
        report_content += self.generate_ai_analysis_report()

        # æµ‹è¯•ç”¨ä¾‹è¡¨æ ¼
        report_content += self.export_to_markdown()

        # AIå¢å¼ºç»Ÿè®¡
        report_content += self.generate_ai_enhanced_statistics()

        # ä¿å­˜æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return filename
