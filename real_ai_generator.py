#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸæ­£çš„AIå¢å¼ºåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
é›†æˆçœŸå®çš„AIå¤§æ¨¡å‹APIï¼Œæ”¯æŒå¤šç§AIæœåŠ¡
"""

import os
import json
import requests
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import openai
from ai_test_generator import AITestCaseGenerator, AIAnalysisResult, TestCase, Priority, TestMethod

class AIProvider(Enum):
    """AIæœåŠ¡æä¾›å•†"""
    OPENAI = "openai"
    AZURE_OPENAI = "azure_openai"
    ANTHROPIC = "anthropic"
    GOOGLE_GEMINI = "google_gemini"
    BAIDU_ERNIE = "baidu_ernie"
    ALIBABA_QWEN = "alibaba_qwen"
    ZHIPU_GLM = "zhipu_glm"
    MOONSHOT = "moonshot"

@dataclass
class AIConfig:
    """AIé…ç½®"""
    provider: AIProvider
    api_key: str
    base_url: Optional[str] = None
    model: Optional[str] = None
    max_tokens: int = 4000
    temperature: float = 0.7
    timeout: int = 30

class RealAITestCaseGenerator(AITestCaseGenerator):
    """çœŸæ­£çš„AIå¢å¼ºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨"""
    
    def __init__(self, ai_config: AIConfig, custom_headers: Optional[Dict[str, str]] = None):
        super().__init__(custom_headers)
        self.ai_config = ai_config
        self.setup_ai_client()
        
    def setup_ai_client(self):
        """è®¾ç½®AIå®¢æˆ·ç«¯"""
        if self.ai_config.provider == AIProvider.OPENAI:
            openai.api_key = self.ai_config.api_key
            if self.ai_config.base_url:
                openai.api_base = self.ai_config.base_url
            self.model = self.ai_config.model or "gpt-3.5-turbo"
            
        elif self.ai_config.provider == AIProvider.AZURE_OPENAI:
            openai.api_type = "azure"
            openai.api_key = self.ai_config.api_key
            openai.api_base = self.ai_config.base_url
            openai.api_version = "2023-05-15"
            self.model = self.ai_config.model or "gpt-35-turbo"
            
        else:
            # å…¶ä»–AIæœåŠ¡ä½¿ç”¨HTTPè¯·æ±‚
            self.model = self.ai_config.model or "default"
    
    def call_ai_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨AI API"""
        try:
            if self.ai_config.provider in [AIProvider.OPENAI, AIProvider.AZURE_OPENAI]:
                return self._call_openai_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.ANTHROPIC:
                return self._call_anthropic_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.GOOGLE_GEMINI:
                return self._call_gemini_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.BAIDU_ERNIE:
                return self._call_ernie_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.ALIBABA_QWEN:
                return self._call_qwen_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.ZHIPU_GLM:
                return self._call_glm_api(prompt, system_prompt)
            elif self.ai_config.provider == AIProvider.MOONSHOT:
                return self._call_moonshot_api(prompt, system_prompt)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {self.ai_config.provider}")
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 401:
                error_msg = f"AI APIè®¤è¯å¤±è´¥ (401): APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
            elif e.response.status_code == 403:
                error_msg = f"AI APIæƒé™ä¸è¶³ (403): æ²¡æœ‰è®¿é—®æƒé™"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
            elif e.response.status_code == 429:
                error_msg = f"AI APIè¯·æ±‚è¿‡äºé¢‘ç¹ (429): è¯·ç¨åé‡è¯•"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
            else:
                error_msg = f"AI API HTTPé”™è¯¯ ({e.response.status_code}): {e}"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
        except requests.exceptions.Timeout:
            error_msg = "AI APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        except requests.exceptions.ConnectionError:
            error_msg = "AI APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"AI APIè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            # é™çº§åˆ°æ¨¡æ‹ŸAIåˆ†æ
            return self._fallback_analysis(prompt)
    
    def _call_openai_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨OpenAI API"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬OpenAI API (>=1.0.0)
            from openai import OpenAI
            client = OpenAI(
                api_key=self.ai_config.api_key,
                base_url=self.ai_config.base_url
            )

            response = client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=min(self.ai_config.max_tokens, 2000),  # é™åˆ¶tokenæ•°é‡
                temperature=self.ai_config.temperature,
                timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
            )
            return response.choices[0].message.content

        except ImportError:
            # é™çº§åˆ°æ—§ç‰ˆæœ¬OpenAI API (<1.0.0)
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                max_tokens=self.ai_config.max_tokens,
                temperature=self.ai_config.temperature,
                timeout=self.ai_config.timeout
            )
            return response.choices[0].message.content
    
    def _call_anthropic_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨Anthropic Claude API"""
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.ai_config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        
        data = {
            "model": self.model or "claude-3-sonnet-20240229",
            "max_tokens": self.ai_config.max_tokens,
            "temperature": self.ai_config.temperature,
            "messages": [{"role": "user", "content": full_prompt}]
        }
        
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]
    
    def _call_gemini_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨Google Gemini API"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model or 'gemini-pro'}:generateContent"
        
        headers = {
            "Content-Type": "application/json",
        }
        
        full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
        
        data = {
            "contents": [{
                "parts": [{"text": full_prompt}]
            }],
            "generationConfig": {
                "temperature": self.ai_config.temperature,
                "maxOutputTokens": self.ai_config.max_tokens
            }
        }
        
        response = requests.post(
            f"{url}?key={self.ai_config.api_key}",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["candidates"][0]["content"]["parts"][0]["text"]
    
    def _call_ernie_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨ç™¾åº¦æ–‡å¿ƒä¸€è¨€API"""
        # é¦–å…ˆè·å–access_token
        token_url = "https://aip.baidubce.com/oauth/2.0/token"
        token_params = {
            "grant_type": "client_credentials",
            "client_id": self.ai_config.api_key,
            "client_secret": self.ai_config.base_url  # è¿™é‡Œç”¨base_urlå­˜å‚¨secret
        }
        
        token_response = requests.post(token_url, params=token_params)
        access_token = token_response.json()["access_token"]
        
        # è°ƒç”¨ERNIE API
        url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{self.model or 'completions_pro'}"
        
        headers = {"Content-Type": "application/json"}
        
        messages = []
        if system_prompt:
            messages.append({"role": "user", "content": system_prompt})
            messages.append({"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚"})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "messages": messages,
            "temperature": self.ai_config.temperature,
            "max_output_tokens": self.ai_config.max_tokens
        }
        
        response = requests.post(
            f"{url}?access_token={access_token}",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["result"]
    
    def _call_qwen_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.ai_config.api_key}"
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": self.model or "qwen-turbo",
            "messages": messages,
            "temperature": self.ai_config.temperature,
            "max_tokens": self.ai_config.max_tokens
        }
        
        response = requests.post(
            self.ai_config.base_url or "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["output"]["choices"][0]["message"]["content"]
    
    def _call_glm_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨æ™ºè°±GLM API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.ai_config.api_key}"
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": self.model or "glm-4",
            "messages": messages,
            "temperature": self.ai_config.temperature,
            "max_tokens": self.ai_config.max_tokens
        }
        
        response = requests.post(
            self.ai_config.base_url or "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _call_moonshot_api(self, prompt: str, system_prompt: str = None) -> str:
        """è°ƒç”¨æœˆä¹‹æš—é¢Moonshot API"""
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.ai_config.api_key}"
        }
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        data = {
            "model": self.model or "moonshot-v1-8k",
            "messages": messages,
            "temperature": self.ai_config.temperature,
            "max_tokens": self.ai_config.max_tokens
        }
        
        response = requests.post(
            self.ai_config.base_url or "https://api.moonshot.cn/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=self.ai_config.timeout
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    
    def _fallback_analysis(self, prompt: str) -> str:
        """é™çº§åˆ†æï¼ˆå½“AI APIå¤±è´¥æ—¶ï¼‰"""
        return "AI APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æç®—æ³•ç”Ÿæˆç»“æœã€‚"
    
    def real_ai_analyze_requirements(self, requirement_text: str) -> AIAnalysisResult:
        """çœŸæ­£çš„AIéœ€æ±‚åˆ†æ"""
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è½¯ä»¶æµ‹è¯•æ¶æ„å¸ˆå’ŒAIä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„éœ€æ±‚æ–‡æ¡£ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

        {
            "complexity_score": 0.75,
            "risk_areas": ["å®‰å…¨é£é™©", "æ€§èƒ½é£é™©"],
            "critical_paths": ["ç”¨æˆ·ç™»å½•è·¯å¾„", "æ”¯ä»˜æµç¨‹"],
            "data_patterns": [
                {"type": "input_validation", "name": "ç”¨æˆ·å", "risk_level": "medium"}
            ],
            "business_rules": [
                {"type": "conditional", "condition": "ç”¨æˆ·åæ­£ç¡®", "action": "å…è®¸ç™»å½•"}
            ],
            "integration_points": ["æ”¯ä»˜æ¥å£", "ç”¨æˆ·ç³»ç»Ÿ"],
            "performance_concerns": ["é«˜å¹¶å‘", "å“åº”æ—¶é—´"],
            "security_risks": ["SQLæ³¨å…¥", "XSSæ”»å‡»"],
            "usability_factors": ["ç•Œé¢å‹å¥½æ€§", "é”™è¯¯æç¤º"]
        }

        è¯·ä»”ç»†åˆ†æéœ€æ±‚æ–‡æ¡£çš„å¤æ‚åº¦ï¼ˆ0-1è¯„åˆ†ï¼‰ã€è¯†åˆ«é£é™©åŒºåŸŸã€æå–å…³é”®è·¯å¾„å’Œä¸šåŠ¡è§„åˆ™ã€‚
        """
        
        analysis_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼š

        {requirement_text}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
        1. å¤æ‚åº¦è¯„åˆ†ï¼ˆ0-1ï¼Œè€ƒè™‘ä¸šåŠ¡è§„åˆ™æ•°é‡ã€é›†æˆç‚¹ã€çŠ¶æ€è½¬æ¢ç­‰ï¼‰
        2. é£é™©åŒºåŸŸè¯†åˆ«ï¼ˆå®‰å…¨ã€æ€§èƒ½ã€æ•°æ®ã€é›†æˆã€å¯ç”¨æ€§é£é™©ï¼‰
        3. å…³é”®ä¸šåŠ¡è·¯å¾„æå–
        4. æ•°æ®æ¨¡å¼å’ŒéªŒè¯è§„åˆ™
        5. ä¸šåŠ¡è§„åˆ™å’Œçº¦æŸæ¡ä»¶
        6. ç³»ç»Ÿé›†æˆç‚¹
        7. æ€§èƒ½å…³æ³¨ç‚¹
        8. å®‰å…¨é£é™©è¯„ä¼°
        9. å¯ç”¨æ€§å› ç´ 

        è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
        """
        
        try:
            ai_response = self.call_ai_api(analysis_prompt, system_prompt)
            
            # å°è¯•è§£æAIè¿”å›çš„JSON
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = ai_response.find('{')
                json_end = ai_response.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    json_str = ai_response[json_start:json_end]
                    analysis_data = json.loads(json_str)
                    
                    # åˆ›å»ºAIAnalysisResultå¯¹è±¡
                    return AIAnalysisResult(
                        complexity_score=analysis_data.get('complexity_score', 0.5),
                        risk_areas=analysis_data.get('risk_areas', []),
                        critical_paths=analysis_data.get('critical_paths', []),
                        data_patterns=analysis_data.get('data_patterns', []),
                        business_rules=analysis_data.get('business_rules', []),
                        integration_points=analysis_data.get('integration_points', []),
                        performance_concerns=analysis_data.get('performance_concerns', []),
                        security_risks=analysis_data.get('security_risks', []),
                        usability_factors=analysis_data.get('usability_factors', [])
                    )
                else:
                    raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"AIè¿”å›ç»“æœè§£æå¤±è´¥: {e}")
                print(f"AIåŸå§‹è¿”å›: {ai_response}")
                # é™çº§åˆ°æœ¬åœ°åˆ†æ
                return super().ai_analyze_requirements(requirement_text)
                
        except Exception as e:
            print(f"AIåˆ†æå¤±è´¥: {e}")
            # é™çº§åˆ°æœ¬åœ°åˆ†æ
            return super().ai_analyze_requirements(requirement_text)
    
    def real_ai_generate_test_cases(self, requirement_text: str, ai_analysis: AIAnalysisResult, 
                                   historical_defects: List[str] = None) -> List[TestCase]:
        """çœŸæ­£çš„AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ"""
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„è½¯ä»¶æµ‹è¯•ä¸“å®¶ã€‚è¯·æ ¹æ®éœ€æ±‚åˆ†æç»“æœç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹ã€‚

        é‡è¦è¯´æ˜ - æ¨¡å—å±‚çº§ç»“æ„ï¼š
        - moduleï¼ˆæ¨¡å—ï¼‰ï¼šæŒ‡ç³»ç»Ÿçš„å¤§åŠŸèƒ½æ¨¡å—ï¼Œå¦‚"é¦–é¡µ"ã€"ä¸ªäººä¿¡æ¯"ã€"è®¢å•ç®¡ç†"ã€"å•†å“ç®¡ç†"ç­‰
        - submoduleï¼ˆå­æ¨¡å—ï¼‰ï¼šæŒ‡å¤§æ¨¡å—ä¸‹çš„å…·ä½“åŠŸèƒ½ï¼Œå¦‚"é¦–é¡µ"ä¸‹çš„"å¯¼èˆªæ "ã€"è½®æ’­å›¾"ï¼›"ä¸ªäººä¿¡æ¯"ä¸‹çš„"åŸºæœ¬ä¿¡æ¯"ã€"è´¦æˆ·è®¾ç½®"ç­‰

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›æµ‹è¯•ç”¨ä¾‹æ•°ç»„ï¼š

        [
          {
            "module": "é¦–é¡µ",
            "submodule": "å¯¼èˆªæ ",
            "case_id": "TC_001",
            "precondition": "ç”¨æˆ·å·²æ‰“å¼€ç½‘ç«™é¦–é¡µ",
            "test_steps": "1. æŸ¥çœ‹é¡µé¢é¡¶éƒ¨å¯¼èˆªæ \n2. ç‚¹å‡»'å•†å“åˆ†ç±»'èœå•\n3. éªŒè¯ä¸‹æ‹‰èœå•æ˜¾ç¤º",
            "expected": "å¯¼èˆªæ æ­£å¸¸æ˜¾ç¤ºï¼Œä¸‹æ‹‰èœå•åŒ…å«æ‰€æœ‰å•†å“åˆ†ç±»",
            "priority": "P1",
            "remark": "å¯¼èˆªåŠŸèƒ½æµ‹è¯•"
          }
        ]

        è¦æ±‚ï¼š
        1. æ ¹æ®éœ€æ±‚å†…å®¹è¯†åˆ«æ­£ç¡®çš„ä¸šåŠ¡æ¨¡å—å±‚çº§
        2. moduleå¿…é¡»æ˜¯ç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½æ¨¡å—
        3. submoduleå¿…é¡»æ˜¯è¯¥æ¨¡å—ä¸‹çš„å…·ä½“å­åŠŸèƒ½
        4. ç”Ÿæˆ3-6ä¸ªé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹
        5. æµ‹è¯•æ­¥éª¤è¦å…·ä½“å¯æ‰§è¡Œ
        6. åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹
        """
        
        # æ„å»ºç®€åŒ–çš„ç”Ÿæˆæç¤º
        generation_prompt = f"""
        è¯·ä¸ºä»¥ä¸‹éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼š

        {requirement_text}

        é‡ç‚¹å…³æ³¨ï¼š
        - æ­£å¸¸åŠŸèƒ½æµç¨‹æµ‹è¯•
        - å¼‚å¸¸æƒ…å†µå¤„ç†
        - è¾¹ç•Œå€¼å’Œè¾“å…¥éªŒè¯
        - ä¸šåŠ¡è§„åˆ™éªŒè¯

        è¯·è¿”å›JSONæ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹æ•°ç»„ã€‚
        """
        
        try:
            ai_response = self.call_ai_api(generation_prompt, system_prompt)
            print(f"âœ… AIç”Ÿæˆå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(ai_response)}")

            # è§£æAIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
            try:
                # å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                try:
                    test_cases_data = json.loads(ai_response)
                    print("âœ… ç›´æ¥JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError:
                    # æå–JSONéƒ¨åˆ†
                    print("ğŸ” å°è¯•æå–JSONéƒ¨åˆ†...")
                    json_start = ai_response.find('[')
                    json_end = ai_response.rfind(']') + 1
                    if json_start == -1:
                        json_start = ai_response.find('{')
                        json_end = ai_response.rfind('}') + 1

                    if json_start != -1 and json_end != -1:
                        json_str = ai_response[json_start:json_end]
                        print(f"ğŸ” æå–çš„JSONé•¿åº¦: {len(json_str)}")
                        test_cases_data = json.loads(json_str)
                        print("âœ… æå–JSONè§£ææˆåŠŸ")
                    else:
                        raise ValueError("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")

                # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„
                if isinstance(test_cases_data, dict):
                    if 'test_cases' in test_cases_data:
                        test_cases_data = test_cases_data['test_cases']
                    else:
                        test_cases_data = [test_cases_data]

                print(f"ğŸ” è§£æåˆ° {len(test_cases_data)} ä¸ªæµ‹è¯•ç”¨ä¾‹æ•°æ®")

                # è½¬æ¢ä¸ºTestCaseå¯¹è±¡
                test_cases = []
                for i, case_data in enumerate(test_cases_data):
                    try:
                        priority_str = case_data.get('priority', 'P1')
                        if priority_str == 'P0':
                            priority = Priority.P0
                        elif priority_str == 'P2':
                            priority = Priority.P2
                        else:
                            priority = Priority.P1

                        test_case = TestCase(
                            module=case_data.get('module', 'AIç”Ÿæˆæ¨¡å—'),
                            submodule=case_data.get('submodule', 'AIç”Ÿæˆå­æ¨¡å—'),
                            case_id=case_data.get('case_id', f'AI_TC_{i+1:03d}'),
                            title=case_data.get('title', f'AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹{i+1}'),
                            precondition=case_data.get('precondition', 'ç³»ç»Ÿæ­£å¸¸è¿è¡Œ'),
                            test_steps=case_data.get('test_steps', 'å¾…è¡¥å……æµ‹è¯•æ­¥éª¤'),
                            expected=case_data.get('expected', 'å¾…è¡¥å……é¢„æœŸç»“æœ'),
                            priority=priority,
                            remark=f"AIç”Ÿæˆ - {case_data.get('remark', 'ä½¿ç”¨çœŸå®AIå¤§æ¨¡å‹ç”Ÿæˆ')}",
                            methods_used=[TestMethod.AI_ENHANCED]
                        )
                        test_cases.append(test_case)
                        print(f"âœ… æˆåŠŸè§£ææµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case.case_id}")
                    except Exception as e:
                        print(f"âŒ è§£ææµ‹è¯•ç”¨ä¾‹ {i} å¤±è´¥: {e}")
                        continue

                if test_cases:
                    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_cases)} ä¸ªAIæµ‹è¯•ç”¨ä¾‹")
                    return test_cases
                else:
                    raise ValueError("æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æµ‹è¯•ç”¨ä¾‹")

            except (json.JSONDecodeError, ValueError) as e:
                print(f"âŒ AIæµ‹è¯•ç”¨ä¾‹è§£æå¤±è´¥: {e}")
                print(f"AIåŸå§‹è¿”å›å‰500å­—ç¬¦: {ai_response[:500]}...")
                print("ğŸ”§ é™çº§åˆ°æœ¬åœ°ç”Ÿæˆ...")
                # é™çº§åˆ°æœ¬åœ°ç”Ÿæˆ
                return super().generate_ai_enhanced_test_cases(requirement_text, historical_defects)

        except Exception as e:
            print(f"âŒ AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {e}")
            print("ğŸ”§ é™çº§åˆ°æœ¬åœ°ç”Ÿæˆ...")
            # é™çº§åˆ°æœ¬åœ°ç”Ÿæˆ
            return super().generate_ai_enhanced_test_cases(requirement_text, historical_defects)
    
    def generate_simple_ai_test_cases(self, requirement_text: str) -> List[TestCase]:
        """ç”Ÿæˆç®€å•çš„AIæµ‹è¯•ç”¨ä¾‹ï¼ˆæ›´å¯é ï¼‰"""

        # åˆ†æéœ€æ±‚æ–‡æœ¬ï¼Œæå–å¯èƒ½çš„æ¨¡å—ä¿¡æ¯
        module_hints = self._extract_module_hints(requirement_text)

        simple_prompt = f"""
        è¯·ä¸ºä»¥ä¸‹åŠŸèƒ½éœ€æ±‚ç”Ÿæˆ5-8ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¿”å›JSONæ•°ç»„æ ¼å¼ï¼š

        éœ€æ±‚ï¼š{requirement_text[:800]}

        æ¨¡å—å±‚çº§è¯´æ˜ï¼š
        - moduleï¼ˆæ¨¡å—ï¼‰ï¼šç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼Œå¦‚"ç”¨æˆ·ç®¡ç†"ã€"å•†å“ç®¡ç†"ã€"è®¢å•ç®¡ç†"ã€"é¦–é¡µ"ã€"ä¸ªäººä¸­å¿ƒ"ç­‰
        - submoduleï¼ˆå­æ¨¡å—ï¼‰ï¼šæ¨¡å—ä¸‹çš„å…·ä½“åŠŸèƒ½ï¼Œå¦‚"ç”¨æˆ·ç®¡ç†"ä¸‹çš„"ç”¨æˆ·æ³¨å†Œ"ã€"ç”¨æˆ·ç™»å½•"ï¼›"é¦–é¡µ"ä¸‹çš„"å¯¼èˆªæ "ã€"è½®æ’­å›¾"ç­‰

        {module_hints}

        è¯·è¿”å›JSONæ•°ç»„æ ¼å¼ï¼š
        [
          {{
            "module": "æ ¹æ®éœ€æ±‚ç¡®å®šçš„ä¸»è¦åŠŸèƒ½æ¨¡å—",
            "submodule": "è¯¥æ¨¡å—ä¸‹çš„å…·ä½“å­åŠŸèƒ½",
            "case_id": "TC_001",
            "title": "ç®€æ´æ˜ç¡®çš„æµ‹è¯•ç”¨ä¾‹æ ‡é¢˜ï¼ˆå¦‚ï¼šéªŒè¯ç”¨æˆ·ç™»å½•çš„æ­£å¸¸æµç¨‹ï¼‰",
            "precondition": "å…·ä½“çš„å‰ç½®æ¡ä»¶",
            "test_steps": "1. å…·ä½“æ“ä½œæ­¥éª¤ä¸€\\n2. å…·ä½“æ“ä½œæ­¥éª¤äºŒ\\n3. å…·ä½“éªŒè¯æ­¥éª¤",
            "expected": "æ˜ç¡®çš„é¢„æœŸç»“æœ",
            "priority": "P1",
            "remark": "æµ‹è¯•æ–¹æ³•è¯´æ˜"
          }}
        ]

        æ ‡é¢˜ç¼–å†™è¦æ±‚ï¼š
        - æ ¼å¼ï¼šéªŒè¯[åŠŸèƒ½/åœºæ™¯]çš„[å…·ä½“è¡Œä¸º]
        - ç¤ºä¾‹ï¼š"éªŒè¯ç”¨æˆ·ç™»å½•çš„æ­£å¸¸æµç¨‹"ã€"éªŒè¯å¯†ç é”™è¯¯æ—¶çš„æç¤ºä¿¡æ¯"ã€"éªŒè¯ç”¨æˆ·åé•¿åº¦è¶…é™çš„è¾¹ç•Œå¤„ç†"

        æµ‹è¯•è¦†ç›–è¦æ±‚ï¼š
        1. æ­£å¸¸æµç¨‹æµ‹è¯• - éªŒè¯ä¸»è¦åŠŸèƒ½çš„æ­£å¸¸æ‰§è¡Œ
        2. è¾¹ç•Œå€¼æµ‹è¯• - éªŒè¯è¾“å…¥å­—æ®µçš„è¾¹ç•Œæ¡ä»¶
        3. å¼‚å¸¸å¤„ç†æµ‹è¯• - éªŒè¯é”™è¯¯è¾“å…¥å’Œå¼‚å¸¸æƒ…å†µ
        4. ä¸šåŠ¡è§„åˆ™æµ‹è¯• - éªŒè¯ç‰¹å®šçš„ä¸šåŠ¡é€»è¾‘
        """

        try:
            print("ğŸ¤– ä½¿ç”¨ç®€åŒ–AIç”Ÿæˆ...")
            ai_response = self.call_ai_api(simple_prompt)
            print(f"âœ… AIå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(ai_response)}")

            # è§£æJSON
            try:
                # æå–JSONéƒ¨åˆ†
                json_start = ai_response.find('[')
                json_end = ai_response.rfind(']') + 1
                if json_start == -1:
                    json_start = ai_response.find('{')
                    json_end = ai_response.rfind('}') + 1

                if json_start != -1 and json_end != -1:
                    json_str = ai_response[json_start:json_end]
                    test_cases_data = json.loads(json_str)

                    if isinstance(test_cases_data, dict):
                        test_cases_data = [test_cases_data]

                    test_cases = []
                    for i, case_data in enumerate(test_cases_data):
                        test_case = TestCase(
                            module=case_data.get('module', 'AIç”Ÿæˆæ¨¡å—'),
                            submodule=case_data.get('submodule', 'AIç”Ÿæˆå­æ¨¡å—'),
                            case_id=case_data.get('case_id', f'AI_TC_{i+1:03d}'),
                            title=case_data.get('title', f'AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹{i+1}'),
                            precondition=case_data.get('precondition', 'ç³»ç»Ÿæ­£å¸¸è¿è¡Œ'),
                            test_steps=case_data.get('test_steps', 'å¾…è¡¥å……æµ‹è¯•æ­¥éª¤'),
                            expected=case_data.get('expected', 'å¾…è¡¥å……é¢„æœŸç»“æœ'),
                            priority=Priority.P1,
                            remark=f"AIç”Ÿæˆ - {case_data.get('remark', 'ä½¿ç”¨çœŸå®AIç”Ÿæˆ')}",
                            methods_used=[TestMethod.AI_ENHANCED]
                        )
                        test_cases.append(test_case)

                    print(f"âœ… æˆåŠŸè§£æ {len(test_cases)} ä¸ªAIæµ‹è¯•ç”¨ä¾‹")
                    return test_cases
                else:
                    print("âŒ æ— æ³•æ‰¾åˆ°JSONæ ¼å¼")
                    return []

            except Exception as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                return []

        except Exception as e:
            print(f"âŒ AIè°ƒç”¨å¤±è´¥: {e}")
            return []

    def generate_real_ai_enhanced_test_cases(self, requirement_text: str,
                                           historical_defects: List[str] = None) -> List[TestCase]:
        """ç”ŸæˆçœŸæ­£çš„AIå¢å¼ºæµ‹è¯•ç”¨ä¾‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        print("ğŸ¤– æ­£åœ¨ä½¿ç”¨çœŸå®AIå¤§æ¨¡å‹åˆ†æéœ€æ±‚...")

        # ç®€åŒ–çš„AIéœ€æ±‚åˆ†æ
        try:
            ai_analysis = self.real_ai_analyze_requirements(requirement_text)
            self.ai_analysis = ai_analysis
            print(f"âœ… AIåˆ†æå®Œæˆ - å¤æ‚åº¦: {ai_analysis.complexity_score:.2f}")
        except Exception as e:
            print(f"âš ï¸  AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ: {e}")
            ai_analysis = super().ai_analyze_requirements(requirement_text)
            self.ai_analysis = ai_analysis

        # å°è¯•ç®€åŒ–çš„AIç”Ÿæˆ
        ai_test_cases = self.generate_simple_ai_test_cases(requirement_text)

        if ai_test_cases:
            print(f"âœ… AIç”ŸæˆæˆåŠŸ - ç”Ÿæˆç”¨ä¾‹: {len(ai_test_cases)}ä¸ª")
        else:
            print("âš ï¸  AIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç”Ÿæˆ")

        # ç»“åˆæœ¬åœ°å¢å¼ºæ–¹æ³•
        print("ğŸ”§ ç»“åˆæœ¬åœ°å¢å¼ºæ–¹æ³•...")
        local_test_cases = super().generate_ai_enhanced_test_cases(requirement_text, historical_defects)

        # åˆå¹¶æµ‹è¯•ç”¨ä¾‹
        all_test_cases = ai_test_cases + local_test_cases

        # å»é‡å’Œä¼˜åŒ–
        final_test_cases = self._deduplicate_and_optimize(all_test_cases)

        self.test_cases = final_test_cases
        print(f"ğŸ‰ æœ€ç»ˆç”Ÿæˆ: {len(final_test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        print(f"   å…¶ä¸­AIç”Ÿæˆ: {len(ai_test_cases)}ä¸ªï¼Œæœ¬åœ°ç”Ÿæˆ: {len(local_test_cases)}ä¸ª")

        return final_test_cases

    def _extract_module_hints(self, requirement_text: str) -> str:
        """ä»éœ€æ±‚æ–‡æœ¬ä¸­æå–æ¨¡å—æç¤º"""
        text_lower = requirement_text.lower()

        # å¸¸è§çš„æ¨¡å—å…³é”®è¯æ˜ å°„
        module_keywords = {
            'ç™»å½•': ('ç”¨æˆ·ç®¡ç†', 'ç”¨æˆ·ç™»å½•'),
            'æ³¨å†Œ': ('ç”¨æˆ·ç®¡ç†', 'ç”¨æˆ·æ³¨å†Œ'),
            'ä¸ªäººä¿¡æ¯': ('ä¸ªäººä¸­å¿ƒ', 'åŸºæœ¬ä¿¡æ¯'),
            'ä¸ªäººä¸­å¿ƒ': ('ä¸ªäººä¸­å¿ƒ', 'ä¸ªäººèµ„æ–™'),
            'è®¾ç½®': ('ä¸ªäººä¸­å¿ƒ', 'è´¦æˆ·è®¾ç½®'),
            'é¦–é¡µ': ('é¦–é¡µ', 'é¡µé¢å±•ç¤º'),
            'å¯¼èˆª': ('é¦–é¡µ', 'å¯¼èˆªæ '),
            'è½®æ’­': ('é¦–é¡µ', 'è½®æ’­å›¾'),
            'æœç´¢': ('æœç´¢åŠŸèƒ½', 'å•†å“æœç´¢'),
            'è´­ç‰©è½¦': ('è´­ç‰©è½¦ç®¡ç†', 'è´­ç‰©è½¦æ“ä½œ'),
            'è®¢å•': ('è®¢å•ç®¡ç†', 'è®¢å•å¤„ç†'),
            'æ”¯ä»˜': ('æ”¯ä»˜ç®¡ç†', 'æ”¯ä»˜å¤„ç†'),
            'å•†å“': ('å•†å“ç®¡ç†', 'å•†å“å±•ç¤º'),
            'åˆ†ç±»': ('å•†å“ç®¡ç†', 'åˆ†ç±»ç®¡ç†'),
            'åº“å­˜': ('åº“å­˜ç®¡ç†', 'åº“å­˜æ§åˆ¶'),
            'è¯„ä»·': ('è¯„ä»·ç®¡ç†', 'å•†å“è¯„ä»·'),
            'æ”¶è—': ('æ”¶è—ç®¡ç†', 'å•†å“æ”¶è—'),
            'åœ°å€': ('åœ°å€ç®¡ç†', 'æ”¶è´§åœ°å€'),
            'ä¼˜æƒ åˆ¸': ('è¥é”€ç®¡ç†', 'ä¼˜æƒ åˆ¸'),
            'ç§¯åˆ†': ('ç§¯åˆ†ç®¡ç†', 'ç§¯åˆ†æ“ä½œ'),
            'å®¢æœ': ('å®¢æœç³»ç»Ÿ', 'åœ¨çº¿å®¢æœ'),
            'æ¶ˆæ¯': ('æ¶ˆæ¯ä¸­å¿ƒ', 'æ¶ˆæ¯é€šçŸ¥'),
            'åé¦ˆ': ('æ„è§åé¦ˆ', 'ç”¨æˆ·åé¦ˆ'),
            'å¸®åŠ©': ('å¸®åŠ©ä¸­å¿ƒ', 'ä½¿ç”¨å¸®åŠ©'),
            'å…³äº': ('ç³»ç»Ÿä¿¡æ¯', 'å…³äºæˆ‘ä»¬')
        }

        # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
        found_modules = []
        for keyword, (module, submodule) in module_keywords.items():
            if keyword in text_lower:
                found_modules.append(f"- å¯èƒ½çš„æ¨¡å—: {module} -> {submodule}")

        if found_modules:
            return f"æ ¹æ®éœ€æ±‚å†…å®¹ï¼Œå»ºè®®çš„æ¨¡å—å±‚çº§ï¼š\n" + "\n".join(found_modules[:3])
        else:
            return "è¯·æ ¹æ®éœ€æ±‚å†…å®¹ç¡®å®šåˆé€‚çš„æ¨¡å—å±‚çº§ç»“æ„"
